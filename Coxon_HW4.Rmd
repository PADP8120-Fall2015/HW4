
---
title: "Coxon_PADP8120_Homework4"
author: "Fall 2015"
date: "![Creative Commons Attribution License](images/cc-by.png)"
output:
  html_document:
    highlight: pygments
    theme: cerulean
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---


# Homework 4

Guidelines: Homeworks should be clear and legible, with answers clearly indicated and work shown. Homeworks will be given a minus, check, or check plus owing to completion and correctness. You are welcome to work with others but please submit your own work. Your homework must be produced in an R Markdown (.rmd) file submitted via github. If you are having trouble accomplishing this, please refer to the [guide](http://spia.uga.edu/faculty_pages/tyler.scott/teaching/PADP8120_Fall2015/Homeworks/submitting_homework.shtml). 


This homework adapts materials from the work of Michael Lynch (http://spia.uga.edu/faculty_pages/mlynch/) and Matthew Salganik (http://www.princeton.edu/~mjs3/)

## Topics

Topics covered in this homework include:

- Bivariate and multivariate regression
- Regression diagnostics

## Problems

### Problem 1. 

Write a function that emulates the `lm` function in R for a simple (bivariate) regression. Like the `lm` function, your function should be able to estimate and report to the screen `B_k` coefficients, standard errors for these coefficients, and corresponding t-values and p-values. It should also report the residual standard error and $R^2$. Be sure to show your code. Compare your results to the results of the `lm` function on some data of your choosing to verify that things are working correctly. 

###### Formulas
$E(Y|x)$ = $\beta_0 + \beta_1$*x_i    

$\beta_1$ = $\sum_{i=1}^n$ $(x_i - \bar{x})$*$(y_i - \bar{y})$ / $(x_i - \bar{x})^2$
$\beta_0$ = $\bar{y}$ - $\beta_1$*$x_i$

Best estimate of $\sigma^2$ is 
$s^2$ = $\sum_{i=1}^n$ $(y_i - \bar{y})^2$ / $n-2$

From : http://www.stat.cmu.edu/~hseltman/309/Book/chapter9.pdf

```{r}
x = rnorm(100)
y = rnorm(100)

victoria.lm = function(y,x)
{
  b1 = round({sum(x * y) - (1/length(y)) * sum(y)} / {sum(x^2) - (1/length(y)) * sum(x)^2}, 5)
  b0 = round(mean(y) - b1 * mean(x),5)
  SEr = sqrt(sum({(y - (b0 + b1 * x))^2}) /
(length(y) - 2))
  SEb1 = SEr / sqrt(sum({(y - (b0 + b1 * x))^2}))
  SEb0 = (SEr * (1/length(y)) * sum(x^2)) / sqrt(sum({(y - (b0 + b1 * x))^2}))
  print.df = data.frame(coef = round(c(b0,b1),3), SE = round(c(SEb0,SEb1),3))
  print.df$t.obs = round(print.df$coef / print.df$SE,3)
  print.df$p.value = round(2*pt(abs(print.df$t.obs), df = length(x) - 2, lower.tail = FALSE), 3)
  rownames(print.df) = c('Intercept', 'X')
  return(print.df)
}

victoria.lm(y=y, x=x)

summary(lm(y~x))
```

### Problem 2. 

Imagine that you've been urged by the teachers' union to show that higher teacher pay leads to better education outcomes.  Of course, you don't do advocacy research --- you are a seeker of truth --- but you decide to investigate this questions scientifically using data about SAT scores and other educational indicators at the state level.  For now we can pretend that this is the only available data (it comes from John Fox's website). [Read the data documentation](http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-2E/datasets/States.pdf) and use the code below to load it

```{r echo=FALSE,message=FALSE,warnings=FALSE}
library(dplyr)
educ <- read.table("https://raw.githubusercontent.com/vcoxon/HW4/master/input/States.txt", header=TRUE)
# now clean up a bit
educ <- educ %>% rename(sat.verbal = satVerbal, sat.math = satMath, percent.taking = percentTaking, percent.no.hs = percentNoHS, teacher.pay = teacherPay) 
# not good to have state as a rowname
educ$state <- rownames(educ)
rownames(educ) <- NULL
educ <- tbl_df(educ)
```

(@) Make a scatter plot showing the relationship between average teacher pay and average SAT score at the state level.  To do this you might have to create a new variable.  And, overlay a regression line on the plot.
```{r}
library(ggplot2)
library(ggthemes)
 
ggplot(educ,aes(x=teacher.pay,y=sat.math+sat.verbal)) + geom_point() +
  geom_smooth(method=lm) + theme_few() + scale_x_continuous(expand=c(0,0))
```

(@) Fit a simple regression model to predict total SAT score based on teacher pay.
```{r}
educ = educ %>% mutate(sat.both = sat.math + sat.verbal)
model = lm(sat.both~teacher.pay,data = educ)
summary(model)
```
(@) Check whether the conditional mean of Y|X is a linear function of X.

*It is a reasonable assumption that Y is a linear funcion of X.  I don't see any obvious curvilinear trends in the scatterplot of the observations from the data.*

(@) Check whether the variance of Y|X is the same for any X. (Check for homo- or heteroskedasticity)  To check for heteroskedasticity, we need to plot residuals against predicted values (i.e., y-hats):
```{r}
plot(model$residuals~predict(model))
```

*It looks like the residuals are more evenly distributed the higher the combined score.  There is a distinct abscence of residuals towards the lower score values.*

(@) Check whether the errors (and thus the Y|X) are independent of one another. (Are the errors iid?)  To check this assumption, we need to plot residuals against the regressor. Itâ€™s probably a good idea to use standardized residuals.
```{r}
plot(rstandard(model)~educ$teacher.pay, ylim = c(-2.5,2.5))
abline(h=0)
```

*This looks like a flipped version of the plot to check for heteroskedasticity.  The "hole" we saw in the earlier plot is here but on the higher end of teacher pay.  I wonder if there is a relationship?*

(@) Check whether the errors are normally distributed with mean zero. (Hint: histogram)
```{r}
hist(model$residuals)
```

*It looks nearly normal/normal enough.*

(@) Identify any outliers and quantify their influence and leverage. 
```{r}
plot(rstandard(model)~educ$teacher.pay,ylim = c(-2.5,2.5))
abline(h=0)
abline(h=2,lty=2)
abline(h=-2,lty=2)
#This places a dashed line at 2 and at -2
```

*Based on these standardized residuals, there doesn't appear to be any high influence or high leverage outliers such that they pull at the line.  There is only one residual that is below the -2nd sd; this would be a normal occurance in a dataset of 51 observations(we could possibly expect at least 2).*

```{r}
summary(round(cooks.distance(model),2))
```

*Cook's Distance doesn't reveal any high leverage values either.*

(@) Explain the substantive conclusion that you would draw from the scatter plot and regression analysis. Be sure to interpret the coefficient in a complete sentence. 
```{r}
summary(model)
```

*This model predicts a decline of 4.8 points on a combined SAT score for every $1,000 increase in teacher pay.*

### Problem 3.

You don't necessarily believe these results, and think there might be more to the story. Thus, you decide to carry on to a multiple regression analysis using more variables.

(@) Using a figure or table, examine the pairwise correlations amongst potential model variables (go ahead and exclude the categorical indicators `state` and `region`. Comment on these results and how they will affect your model fitting. 

```{r}
library(knitr); library(dplyr)
kable(round(cor(educ %>% select(-state,-region)),3))
```

*Looking at the pairwise correlations amongst the variables, it appears that the most promising variables to include in my model would be to regress sat.both on percent.taking and teacher.pay.*

(@) Identify the optimal model(s) using all possible subsets and AIC/BIC.
```{r}
educ.subset = educ %>% dplyr::select(-state,-region,-sat.math,-sat.verbal)
model.list = list(m1 = lm(sat.both~population + percent.taking, data = educ.subset),
                  m2 = lm(sat.both~population + percent.no.hs, data = educ.subset),
                  m3 = lm(sat.both~population + teacher.pay, data = educ.subset),
                  m4 = lm(sat.both~percent.taking + percent.no.hs, data = educ.subset),
                  m5 = lm(sat.both~percent.taking + teacher.pay, data = educ.subset), 
                  m6 = lm(sat.both~percent.no.hs + teacher.pay, data = educ.subset),
                  m7 = lm(sat.both~population + percent.taking + percent.no.hs, data = educ.subset),
                  m8 = lm(sat.both~population + percent.taking + teacher.pay, data = educ.subset),
                  m9 = lm(sat.both~population + percent.no.hs + teacher.pay, data = educ.subset),
                  m10 = lm(sat.both~percent.taking + percent.no.hs + teacher.pay, data = educ.subset),
                  m11 = lm(sat.both~population + percent.taking + percent.no.hs + teacher.pay, data = educ.subset))

model.comps = data.frame(AIC = unlist(lapply(model.list,AIC)),BIC = unlist(lapply(model.list,BIC)),df = unlist(lapply(lapply(model.list,coef),length))-1)
model.comps
```        

*My hunch was partly wrong; the best performing model is m4 with the variables percent.taking and percent.no.hs.  m4's AIC and BIC scores are the lowest and it is parsimonious; it appears to be the best choice from the options available.*    

(@) Identify the optimal model(s) using backward elimination and AIC/BIC.
```{r}
summary(lm.unrestricted <- lm(sat.both~., data = educ.subset))

#REMEMBER: AIC = 2k -2ln(L); k=2
backAIC <- step(lm.unrestricted,direction = 'backward', k = 2)

#REMEMBER: BIC = -2 * loglikelihood + d * log(N) 

#where N is the sample size of the training set and d is the total number of parameters. The lower BIC score signals a better model. From http://stanfordphd.com/BIC.html 
backBIC <- step(lm.unrestricted,direction = 'backward',k = log(nrow(educ.subset)))
```

*Backward elimination and AIC/BIC all indicate that the best/optimal model includes percent.taking and percent.no.hs (the m4 model).* 

(@) Identify the optimal model(s) using forward selection and AIC/BIC.
```{r}
summary(lm.unrestricted <- lm(sat.both~., data = educ.subset))

summary(lm.restricted <- lm(sat.both ~1, data = educ.subset))

#Forward AIC...
step(lm.restricted, scope=list(lower=lm.restricted, upper=lm.unrestricted), direction="forward", k = 2)

# Forward BIC
step(lm.restricted, scope=list(lower=lm.restricted, upper=lm.unrestricted), direction="forward", k = log(nrow(educ.subset)))

```

(@) Do the methods agree on the optimal model?  If not, why not?

*It appears that forward selection using the AIC/BIC also reveal that the optimal model is still m4 with sat.both regressed on variables percent.taking and percent.no.hs.*
$E(sat.both|X_1,X_2)$ = $\beta_0$ + $\widehat{\beta}_{percent.taking} X_1$  + $\widehat{\beta}_{percent.no.hs} X_2$ 

(@) Assess whether your model is doing a good job of modeling the response (hint: think $Y$ vs. $\hat{Y}$  plot).
```{r}
best.model = lm(sat.both ~ percent.taking + percent.no.hs, data = educ.subset)
plot(best.model$fitted.values ~ educ.subset$sat.both,ylim=c(800,1300), xlim=c(800,1300))
```

*It appears to have a discernible positive linear relationship, but I detect a slight curvilinear trend between the outcome, sat.both, and these explanatory variables (percent.taking + percent.no.hs) included in the model.*

(@) Assess the relationship between each each predictor and the response (hint: marginal model plots). Is your model well-specified?
```{r}
#install.packages("car", repos="http://R-Forge.R-project.org")
library(car)
mmp(best.model, educ.subset$percent.taking)
```

*Our best model fits percent.taking as a linear variable.  It appears that our "best"" model is not well-specified considering the percent.taking variable; a curvilinear relationship exists between this variable and sat.both.*

```{r}
#now let's look at percent.no.hs...
mmp(best.model, educ.subset$percent.no.hs)
```

*Compared to percent.taking, this model looks like it fits what we see in the data; the data line and the model line are more closely symmetrical.*

(@) Assess how much a given predictor $x_i$ can explain the response after the other predictors have been taken into account.
```{r}
avPlots(best.model)
```

*Holding all other variables constant, the percent.taking variable (even though it is poorly defined as a  linear variable in this model) explains much of the variation in the outcome variable, sat.both.  Holding all other variables constant, a strong negative linear trend is still apparent after the sat.both is regressed on the percent.no.hs. variable.  Both of these variables make a significant predictive contribution.*

(@) Recommend a final model and provide your reasoning.

*The m4 model $E(sat.both|X_1,X_2)$ = $\beta_0$ + $\widehat{\beta}_{percent.taking} X_1$  + $\widehat{\beta}_{percent.no.hs} X_2$  is the most logical model to go with considering that it passed all three fitting strategies (Stepwise,Forward/Backward AIC & BIC).*

*It this data set, we only have four variables to work with (population, percent.taking, percent.no.hs, & teacher.pay); I don't see the connection between population and sat.both.  Reformers discuss teacher pay as having a direct effect on the test scores of students, but it is established knowledge that SAT scores are more correlated with a student's socioeconomic status (SES) than with teacher pay.*

*That being said, percent.taking (higher percentages of students taking the SAT indicate pockets of higher SES because low-income students refrain due to financial constraints) and percent.no.hs are rough measures of SES, but they are better than nothing and allow us to incorporate SES as an explanatory  variable into our model.  In short, our model's construction makes sense and is supported in the literature.*  

(@) Provide an interpretation (using sentences as you might in an academic journal) of your coefficient results.
```{r}
summary(best.model)
```

*The association between the percentage of students in each state who take the SAT and the average SAT score is negative and statistically significant at 0.001 significance. For every one percentage point increase in the number of students taking the SAT, the average score is predicted to decrease by 2.34 points.*

*The predicted association between the percentage of state residents without a high school degree and the state average SAT score is also negative and statistically significant at the 0.01 level. For every one percentage point increase in a state's percentage of residents without a high school diploma, our model predicts a 2.54 point decrease in average score.*

### Problem 4.

Examine Angellâ€™s data on the moral integration of U.S. cities (Angells is a data file in the car library). 

```{r message=FALSE,warnings=FALSE}
library(car)
data("Angell")
head(Angell)
```

(@) Regress moral integration on heterogeneity and geographic mobility for the cities in data set (multiple regression). 
```{r}
model_1 = lm(moral ~ hetero + mobility, data = Angell)
summary(model_1)
```

(@) Report the finding of the results. Be sure to use a table to report $\beta_0$, $\beta_1$, and $\beta_2$ and statistics that allow for significance tests to be performed on these three coefficients. Write a paragraph to substantively explain the results of the model.
```{r}
#install.packages("texreg", repos="http://R-Forge.R-project.org")
library(texreg)
screenreg(model_1)
avPlots(model_1)
```

*The results of this model indicate that both heterogeneity and mobility are negatively related to moral integration in cities.* 

*Both independent variables are found to have statistically significant (at the 0.001 level), negative assocations with moral integration. For every one-unit increase in heterogeneity moral integration is predicted to be reduced moral by 0.11 units. For every one-unit increase in mobility moral integration is predicted to be reduced moral by 0.19 units.*

### Report your process

You're encouraged to reflect on what was hard/easy, problems you solved, helpful tutorials you read, etc. Give credit to your sources, whether it's a blog post, a fellow student, an online tutorial, etc.

### Rubric

Minus: Didn't tackle at least 3 tasks. Or didn't make companion graphs. Didn't interpret anything but left it all to the "reader". Or more than one technical problem that is relatively easy to fix. It's hard to find the report in our repo.

Check: Completed, but not fully accurate and/or readable. Requires a bit of detective work on my part to see what you did

Check plus: Hits all the elements. No obvious mistakes. Pleasant to read. No heroic detective work required. Solid.




#### The command below is helpful for debugging, please don't change it

```{r echo=FALSE}
sessionInfo()
```









